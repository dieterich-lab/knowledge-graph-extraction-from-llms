{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2672fab1-965a-47d2-95f8-b4f6c94137b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import tiktoken\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b437c91a",
   "metadata": {},
   "source": [
    "## Step 1: convert the pdf to text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200bf06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pdf_to_text(pdf_path, output_folder):\n",
    "    \"\"\"\n",
    "    Converts a PDF file to a text file by extracting the text from each page of the PDF.\n",
    "\n",
    "    This function opens a PDF file, reads its contents, and extracts text from each page. \n",
    "    The extracted text is then saved into a new text file. The name of the output text file is the same \n",
    "    as the input PDF file, but with a .txt extension. The output text file is saved in the specified \n",
    "    output folder.\n",
    "\n",
    "    Parameters:\n",
    "    pdf_path: The path to the PDF file that needs to be converted. This can be a string or a pathlib.Path object.\n",
    "    output_folder: The path to the folder where the output text file will be saved. This can be a string or a pathlib.Path object.\n",
    "\n",
    "    Returns:\n",
    "    None: This function doesn't return anything. However, it prints a message with the path of the \n",
    "          generated text file after successfully converting and saving the text.\n",
    "    \n",
    "    \"\"\"\n",
    "    pdf_file_path = Path(pdf_path)\n",
    "    output_folder_path = Path(output_folder)\n",
    "\n",
    "    with pdf_file_path.open('rb') as pdf_file:\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "\n",
    "        # Extract text from each page\n",
    "        text = ''\n",
    "        for page in pdf_reader.pages:\n",
    "            if page.extract_text() is not None:\n",
    "                text += page.extract_text()\n",
    "\n",
    "        # Remove extension from the PDF filename and create output file path\n",
    "        output_file = output_folder_path / f'{pdf_file_path.stem}.txt'\n",
    "\n",
    "        # Save the extracted text to a text file\n",
    "        with output_file.open('w') as txt_file:\n",
    "            txt_file.write(text)\n",
    "\n",
    "        print(f'PDF converted to text and saved at: {output_file}')\n",
    "\n",
    "pdf_folder_path = ''  # Specify the PDF folder path\n",
    "output_folder_path = ''  # Specify the output folder path\n",
    "\n",
    "# Iterate over PDF files in the folder\n",
    "pdf_folder = Path(pdf_folder_path)\n",
    "for pdf_file in pdf_folder.glob('*.pdf'):\n",
    "    convert_pdf_to_text(pdf_file, output_folder_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f831726",
   "metadata": {},
   "source": [
    "## Step 2: clean some parts of the text files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41378ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_file(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Cleans a text file by removing specific patterns and phrases, and saves the cleaned text to a new file.\n",
    "    After cleaning the text, the function writes the cleaned text to the specified output file.\n",
    "\n",
    "    Parameters:\n",
    "    input_file (str): The path to the text file that needs to be cleaned.\n",
    "    output_file (str): The path to the file where the cleaned text will be saved.\n",
    "\n",
    "    Returns:\n",
    "    None: This function doesn't return anything but prints a message with the path of the \n",
    "          output file after successfully cleaning and saving the text.\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(input_file, 'r') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Remove references like [95] or [88, 89] or [107 –109]\n",
    "    # text = re.sub(r'\\[\\d+(?:[-,]\\d+)?\\]', '', text)\n",
    "    text = re.sub(r'\\[\\d+(\\s*[,-]\\s*\\d+)?\\]', '', text)\n",
    "\n",
    "    # Connect words separated by a dash on different lines\n",
    "    text = re.sub(r'(\\w+)-\\n(\\w+)', r'\\1\\2', text)\n",
    "    \n",
    "    # Remove the 4-digit number before \"Roeb E et al.\"\n",
    "    text = re.sub(r'\\d{4} Roeb E et al.', 'Roeb E et al.', text)\n",
    "\n",
    "    # Remove specific phrases\n",
    "    phrases_to_remove = [\n",
    "        \"Roeb E et al. Aktualisierte S2k-Leitlinie nicht-alkoholische …Z Gastroenterol 2022; 60: 1346 –1421 | © 2022. Thieme. All rights reserved. Leitlinie\",\n",
    "        \"Heruntergeladen von: Nadine Fischer. Urheberrechtlich geschützt.\",\n",
    "        \"Roeb E et al. Aktualisierte S2k-Leitlinie nicht-alkoholische …Z Gastroenterol 2022; 60: 1346 –1421 | © 2022. Thieme.  All rights reserved. Heruntergeladen von: Nadine Fischer. Urheberrechtlich geschützt\",\n",
    "        \"Roeb E et al. Aktualisierte S2k-Leitlinie nicht-alkoholische …Z Gastroenterol 2022; 60: 1346 –1421 | © 2022. Thieme. All rights reserved.Leitlinie\"\n",
    "        \"Morbus Fabry – Leitlinien für Diagnostik und Therapie in der Neurologie\",\n",
    "        \"Leitlinien für Diagnostik und Therapie in der Neurologie © DGN 2023\",\n",
    "        \"Leitlinien für Diagnostik und Therapie in der Neurologie\"\n",
    "    ]\n",
    "    for phrase in phrases_to_remove:\n",
    "        text = text.replace(phrase, '')\n",
    "\n",
    "    \n",
    "\n",
    "    # Write the cleaned text to the output file\n",
    "    with open(output_file, 'w') as file:\n",
    "        file.write(text)\n",
    "\n",
    "    print(f\"Text file cleaned and saved at: {output_file}\")\n",
    "\n",
    "\n",
    "# Specify the input text file and the output file path\n",
    "input_file_path = ''\n",
    "output_file_path = ''\n",
    "\n",
    "# Call the function to clean the text file\n",
    "clean_text_file(input_file_path, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d21eded",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13277ec7",
   "metadata": {},
   "source": [
    "## Step 3: Fix the spacing issue in the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8ba4885-2ed5-475b-811f-500782c45a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"\" #add your API key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d20a626-b844-4533-806a-e49874fe2bdb",
   "metadata": {},
   "source": [
    "### Create blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7050f786-f2aa-44d5-ace6-74a597d2a81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\"):\n",
    "    \"\"\" \n",
    "    Calculates the total number of tokens used by a list of messages based on a specific model's encoding.\n",
    "\n",
    "    This function iterates over a list of messages and computes the total number\n",
    "    of tokens required to encode these messages using a specified model's encoding scheme. \n",
    "\n",
    "    If the specified model's encoding is not found, it defaults to using the 'cl100k_base' encoding and\n",
    "    issues a warning.\n",
    "\n",
    "    Parameters:\n",
    "    messages (list of dicts): A list of messages, where each message is a dictionary containing key-value pairs.\n",
    "                              The keys represent different parts of the message, like 'name', 'text', etc.\n",
    "    model (str): The model name which defines the encoding scheme to be used. Default is 'gpt-3.5-turbo-0613'.\n",
    "\n",
    "    Returns:\n",
    "    int: The total number of tokens required to encode all the messages in the list according to the specified\n",
    "         model's encoding.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    tokens_per_message = 3\n",
    "    tokens_per_name = 1\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            try:\n",
    "                num_tokens += len(encoding.encode(value))\n",
    "            except TypeError:\n",
    "                print(f\"{value=}\")\n",
    "                raise\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "        num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66b6868c-5c0d-40de-86fc-6bdf70976c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string, model=\"gpt-3.5-turbo-0613\") -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d29c1a7-10d2-4285-96b6-c2888bedde9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_blocks(text: str, model=\"gpt-3.5-turbo-0613\", max_tokens=2000):\n",
    "    \"\"\"\n",
    "    Splits a given text into smaller blocks based on token count limits for a specific model's encoding.\n",
    "\n",
    "    The function divides the text into sentences and then groups these sentences into blocks. \n",
    "    Each block's total token count is kept under a specified maximum limit.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The input text to be split into blocks.\n",
    "    model (str): The model name that defines the encoding scheme for counting tokens. Default is 'gpt-3.5-turbo-0613'.\n",
    "    max_tokens (int): The maximum number of tokens allowed in each block. Default is 2000.\n",
    "\n",
    "    Returns:\n",
    "    list of str: A list where each element is a string block of the original text. \n",
    "                 Each block contains sentences from the text and has a token count less than or equal to `max_tokens`.\n",
    "    \"\"\"\n",
    "    # Split the text into sentences by full stop\n",
    "    sentences = text.split('.')\n",
    "\n",
    "    text_blocks = []\n",
    "    current_block = []\n",
    "\n",
    "    tokens_in_block = 50\n",
    "    for sentence in sentences:\n",
    "        # Add the full stop back to the sentence and a space\n",
    "        sentence = sentence.strip() + '. '\n",
    "\n",
    "        sentence_tokens = num_tokens_from_string(sentence, model)\n",
    "        if tokens_in_block + sentence_tokens > max_tokens:\n",
    "            # If adding the next sentence exceeds the limit, finalize the current block and start a new one\n",
    "            text_blocks.append(\"\".join(current_block))\n",
    "            current_block = [sentence]\n",
    "            tokens_in_block = 50 + sentence_tokens\n",
    "        else:\n",
    "            current_block.append(sentence)\n",
    "            tokens_in_block += sentence_tokens\n",
    "\n",
    "    # Don't forget the last block\n",
    "    if current_block:\n",
    "        text_blocks.append(\"\".join(current_block))\n",
    "\n",
    "    return text_blocks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93bff8f-ca34-457b-bdbb-8244fe8b8a20",
   "metadata": {},
   "source": [
    "### Get files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd32b908-5cb4-4c41-a8ff-0dc958e54b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "step1 = [f.name for f in Path('text_step1').glob(\"*.txt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84ff7b4d-c83a-4f06-8e92-82f9cb114dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "step2 = [f.name for f in Path('text_step2').glob(\"*.txt\")]\n",
    "already_corrected = [f.name for f in Path('text_step2').glob(\"*.txt\") if f.name in step1]\n",
    "assert not len(already_corrected), f\"{already_corrected} already corrected\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c734eeae-5c36-46d4-9993-29e7d2ee9035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['051-001l_S3_Funktionelle_Koerperbeschwerden_2018-11.txt',\n",
       " '065-002l_S2k_Venenthrombose-Lungenembolie_2023-03.txt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step1_filtered = [f for f in step1 if f not in already_corrected]\n",
    "step1_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e5ef32-2c71-4c2a-9ffb-4851455c7229",
   "metadata": {},
   "source": [
    "# construct prompt and call API in loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8f5e601-414e-4d71-a595-12c20fb4d081",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Your task is to correct the spacing errors in the following German text. Please ensure that you do not add, delete, or rearrange any words. Only adjust the spaces between words as necessary.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02dc21ad-1ae2-4194-8fa2-8e5d874d7b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "skipped = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4f1f1bd-5058-4916-b391-16b5c8b91ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['051-001l_S3_Funktionelle_Koerperbeschwerden_2018-11.txt',\n",
       " '065-002l_S2k_Venenthrombose-Lungenembolie_2023-03.txt']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step1_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f28da9db-cd77-4c06-9cf5-713d7a17dd7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f='051-001l_S3_Funktionelle_Koerperbeschwerden_2018-11.txt' results in 65 API calls.\n",
      "Expected cost: 91.00 cent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "65it [02:18,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "f='065-002l_S2k_Venenthrombose-Lungenembolie_2023-03.txt' results in 84 API calls.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for f in step1_filtered:\n",
    "    # read file and split into blocks\n",
    "    with open(f\"text_step1/{f}\", \"r\") as fh:\n",
    "        text = ' '.join(fh.readlines()).replace('\\n', ' ')\n",
    "    blocks = split_text_into_blocks(text)\n",
    "\n",
    "    # construct prompts\n",
    "    prompts = []\n",
    "    for block in blocks:\n",
    "        _prompt = [{\n",
    "            \"role\": \"system\",\n",
    "            \"content\": prompt\n",
    "        },\n",
    "                   {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": block\n",
    "        }]\n",
    "        num_token = num_tokens_from_messages(_prompt)\n",
    "        assert num_token < 2048\n",
    "        prompts.append(_prompt)\n",
    "    \n",
    "    print(f\"{f=} results in {len(prompts)} API calls.\")\n",
    "    if len(prompts) > 70:\n",
    "        skipped.append(f)\n",
    "        continue\n",
    "    expected_cost = len(prompts) * (4 * (0.0015 + 0.002)) * 100\n",
    "    print(f\"Expected cost: {expected_cost:.2f} cent\")\n",
    "    \n",
    "    # call API\n",
    "    completions = []\n",
    "    for i, p in tqdm(enumerate(prompts)):\n",
    "        stem, suffix = Path(f).stem, Path(f).suffix\n",
    "        part_number = str(i + 1).zfill(3)\n",
    "        part_file = Path(f'parts/{stem}_{part_number}{suffix}')\n",
    "        if Path(part_file).is_file():\n",
    "            with open(part_file, 'r') as fh:\n",
    "                content = fh.read()\n",
    "        else:\n",
    "            completion = openai.ChatCompletion.create(\n",
    "              model=\"gpt-3.5-turbo-0613\",\n",
    "              messages=p,\n",
    "              max_tokens=2048  # probably 4096\n",
    "            )\n",
    "            content = completion.choices[0].message['content']\n",
    "            with open(part_file, 'w') as fh:\n",
    "                fh.write(content)\n",
    "\n",
    "        completions.append(content)\n",
    "        \n",
    "    # write to file in step2\n",
    "    full_text = '\\n'.join(content for content in completions)\n",
    "    with open(f'text_step2/{f}', 'w') as fh:\n",
    "        fh.write(full_text)\n",
    "\n",
    "    # delete part files\n",
    "    part_files = [part for part in Path('parts').glob(\"*.txt\")]\n",
    "    for p in part_files: p.unlink()\n",
    "    print('done')\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a82e3f0-e726-4e4b-8bd5-b4818e96909e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e128bc-45df-47eb-a55e-3383cb57dca8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5520ca0-e66e-48c7-a354-16846088e3ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1076f7ee-bcab-4c24-8fd8-0f9b9066490d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338b1e3e-5e10-48d0-a9c8-2c50550f4522",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
